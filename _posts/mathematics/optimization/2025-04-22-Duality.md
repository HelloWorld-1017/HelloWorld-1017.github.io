---
title: An Example of Duality in Optimization
categories:
 - Mathematics
tags:
 - Optimization
date: 2025-04-22 10:13:58 +0800
last_modified_at: 2025-04-26 12:51:40 +0800
---

# Primal optimization problem vs. dual optimization problem

Consider such a case (obtained from reference[^1]). In a university course, students should complete and submit several homeworks throughout the whole semester. Right now, there is a course where every homework that students should complete includes five problems, and after students submitting their solutions of these problems, TA will score them. One special aspect of this case, however, is that, TA will only grade one out of five submitted problems.

Generally, students and TAs hold opposite positions: for a submitted homework,

- The objective of a student is to make the lost points as less as possible
- The objective of a TA is to make the lost points as many as possible (suppose that TA is very strict)

In this example, there are three types of strategies that students may adopt, and corresponding expected lost points of each case show as follows:

- Strategy 1: the student assigns equal effort into all five problems, and the lost points of five problems are expected as $[5,5,5,5,5]$.
- Strategy 2: the student invests more time into one problem than other four problems. For example, if the student chooses to spend more time on the first problem, and then the lost points will be $[1,8,8,8,8]$.
- Strategy 3: the student skips four problems (doesn’t respond at all) and spends all time on one problem to guarantee no points lost on the selected one. For example, if the student spends all time on the first problem, then the lost points of each problem are expected to be $[0,+\infty,+\infty,+\infty,+\infty]$, where lost points being $+\infty$ denotes automatic failure of this homework.

Let $f(x,y)$ denotes the expected lost points in the situation where the student adopts strategy $x$ (strategy 1, 2, or 3) and TA chooses to grade the problem $y$, then we can write two optimization problems.

**Optimization problem 1 (from the perspective of student)**: Because the student would like to make the lost points of homework as less as possible, so the student should decide to adopt which strategy $x$ by the optimization:

$$
\min_x f(x,y)\notag
$$

However, on the other hand, the student doesn’t know the TA will grade which problem, so he’d better consider the worst case of adopting each strategy $x$ (if you were a student, you should consider that, if you chose strategy $x$, how many points will be lost at most? As a student, we should minimize it.), that is:

$$
\min_x\max_yf(x,y)\label{eq1}
$$

**Optimization problem 2 (from the perspective of TA)**: Conversely, the TA would like to make the lost points of submitted homework as many as possible, so the TA should decide to grade which problem $y$ by the optimization:

$$
\max_y f(x,y)\notag
$$

However, similarly, the TA doesn’t know which strategy the student will adopt, so he should consider the worst case of grading each problem $y$ (if you were a TA, you should consider that, if you grade problem $y$, how many points will be lost at least? As a TA, we should maximize it.) that is:

$$
\max_y\min_xf(x,y)\label{eq2}
$$

Optimization problems $\eqref{eq1}$ and $\eqref{eq2}$ have the same possible values of $f(x,y)$, or saying feasible region, and we can list them in a matrix form:

$$
\begin{split}
\boldsymbol{\mathrm{A}}&=
\begin{bmatrix}
f(1,1) & f(1,2) & f(1,3) & f(1,4) & f(1,5)\\
f(2,1)_1 & f(2,2)_1 & f(2,3)_1 & f(2,4)_1 & f(2,5)_1\\
f(2,1)_2 & f(2,2)_2 & f(2,3)_2 & f(2,4)_2 & f(2,5)_2\\
f(2,1)_3 & f(2,2)_3 & f(2,3)_3 & f(2,4)_3 & f(2,5)_3\\
f(2,1)_4 & f(2,2)_4 & f(2,3)_4 & f(2,4)_4 & f(2,5)_4\\
f(2,1)_5 & f(2,2)_5 & f(2,3)_5 & f(2,4)_5 & f(2,5)_5\\
f(3,1)_1 & f(3,2)_1 & f(3,3)_1 & f(3,4)_1 & f(3,5)_1\\
f(3,1)_2 & f(3,2)_2 & f(3,3)_2 & f(3,4)_2 & f(3,5)_2\\
f(3,1)_3 & f(3,2)_3 & f(3,3)_3 & f(3,4)_3 & f(3,5)_3\\
f(3,1)_4 & f(3,2)_4 & f(3,3)_4 & f(3,4)_4 & f(3,5)_4\\
f(3,1)_5 & f(3,2)_5 & f(3,3)_5 & f(3,4)_5 & f(3,5)_5\\
\end{bmatrix}\\
&=
\begin{bmatrix}
5 & 5 & 5 & 5 & 5\\
1 & 8 & 8 & 8 & 8\\
8 & 1 & 8 & 8 & 8\\
8 & 8 & 1 & 8 & 8\\
8 & 8 & 8 & 1 & 8\\
8 & 8 & 8 & 8 & 1\\
0 & +\infty & +\infty & +\infty & +\infty\\
+\infty & 0 & +\infty & +\infty & +\infty\\
+\infty & +\infty & 0 & +\infty & +\infty\\
+\infty & +\infty & +\infty & 0 & +\infty\\
+\infty & +\infty & +\infty & +\infty & 0\\
\end{bmatrix}
\end{split}\notag
$$

Note: From 2nd to 6th rows, $i$ in $f(\cdot,\cdot)_i$ represents that the student spends more time and effort on $i$-th problem when adopting Strategy 2; whereas from 7th to 11th rows, it represents that the student spends all time on $i$-th problem when adopting Strategy 3.
{: .notice--primary}

Then, to find the solution of optimization $\eqref{eq1}$, we could firstly find the maximum of each row:

$$
\max_y f(x,y)=\{5,8,8,8,8,8,+\infty,+\infty,+\infty,+\infty,+\infty\}\notag
$$

and among which find the minimum $p^*$, i.e.

$$
p^* = \min_x\{5,8,8,8,8,8,+\infty,+\infty,+\infty,+\infty,+\infty\}= 5\label{eq3}
$$

Similarly, for $\eqref{eq2}$, we could find the minimum of each column:

$$
\min_xf(x,y) = \{0,0,0,0,0\}\notag
$$

and among which find the maximum $d^*$, i.e.

$$
d^*=\max_y\{0,0,0,0,0\}=0\label{eq4}
$$

<br>

In the above example, model $\eqref{eq1}$ is called the <i class="term">primal optimization problem</i>, whereas $\eqref{eq2}$ is called the <i class="term">dual optimization problem</i>. The Wikipedia article[^2] provides a simple description about them:

<div class="quote--left" markdown="1">

In mathematical optimization theory, duality or the duality principle is the principle that optimization problems may be <i class="emphasize">viewed from either of two perspectives</i>, the primal problem or the dual problem. <i class="emphasize">If the primal is a minimization problem then the dual is a maximization problem (and vice versa)</i>.

</div>

<br>

# Weak duality

There is an important relation between the optimal value of the primal optimization problem $\eqref{eq3}$ and that of dual optimization problem $\eqref{eq4}$, that is:

$$
d^*\le p^*\label{eq5}
$$

We can understand this point from the perspective of student: if the student knows which problem TA plans to grade, he will lose less points.

Actually, inequality $\eqref{eq5}$ expresses the <i class="term">weak duality</i>.

<div class="quote--left" markdown="1">

Weak duality: For any matrix $\boldsymbol{\mathrm{A}}=(a_{ij})\in\mathbb{R}^{m\times n}$, it is always the case that[^1]

$$
d^*\le p^*\label{eq6}
$$

</div>

<div class="quote--left" markdown="1">

Any feasible solution to the primal (<i class="emphasize">minimization</i>) problem is at least as large as any feasible solution to the dual (<i class="emphasize">maximization</i>) problem. Therefore, <i class="emphasize">the solution to the primal is an upper bound to the solution of the dual, and the solution of the dual is a lower bound to the solution of the primal</i>. This fact is called weak duality.[^2]

</div>


Here is a proof of $\eqref{eq6}$:

Because

$$
p^*=\min_i\max_jf(i,j)=a_{i_pj_p}\ge a_{i_p,\forall j}\ge a_{i_pj_d}\notag
$$

and

$$
d^*=\max_j\min_if(i,j)=a_{i_dj_d}\le a_{\forall i,j_d}\le a_{i_pj_d}\notag
$$

i.e., we have:

$$
p^*\ge a_{i_pj_d}\ge d^*\notag
$$

<br>

**References**

[^1]: [Review Notes and Supplementary Notes CS229 Course Machine Learning Standford University](https://www.ctanujit.org/uploads/2/5/3/9/25393293/mathematics_for_machine_learning__cs229__1.pdf), Convex Optimization Overview (cnt’d), Chuong B. Do, October 26, 2007, pp. 1-5.
[^2]: [Duality (optimization)](https://en.wikipedia.org/wiki/Duality_\(optimization\)).
