---
layout: single
title: Some Terminology and Methods in the Field of Large Language Models
categories:
 - Machine Learning
 - Collections
tags:
 - NLP (Natural Language Processing)
toc: false
date: 2024-01-15 20:35:28 +0800
last_modified_at0: 2024-01-15 20:35:28 +0800
last_modified_at1: 2024-03-22 21:19:30 +0800
last_modified_at2: 2024-03-23 23:58:47 +0800
last_modified_at3: 2024-03-26 09:17:19 +0800
last_modified_at4: 2024-04-26 11:17:50 +0800
last_modified_at5: 2024-05-10 23:12:36 +0800
last_modified_at6: 2024-05-21 17:38:46 +0800
last_modified_at7: 2024-05-22 06:25:50 +0800
last_modified_at8: 2024-05-23 08:11:35 +0800
last_modified_at9: 2024-05-29 10:01:26 +0800
last_modified_at: 2024-05-29 10:01:26 +0800
---

- **LLM (Large Language Model)**
  - [What are Large Language Models? - AWS](https://aws.amazon.com/what-is/large-language-model/).
  -  [Large language model - Wikipedia](https://en.wikipedia.org/wiki/Large_language_model).
- **NLP (Natural Language Processing)**
  - [Natural language processing - Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing).
- **NLU (Natural Language Understanding)** or **NLI (Natural Language Interpretation)**
  - [What is Natural Language Understanding (NLU)? - Definition from TechTarget](https://www.techtarget.com/searchenterpriseai/definition/natural-language-understanding-NLU).
  - [Natural-language understanding - Wikipedia](https://en.wikipedia.org/wiki/Natural-language_understanding).
- **NLG (Natural Language Generation)**
  - [What is Natural Language Generation (NLG)? - Definition from TechTarget](https://www.techtarget.com/searchenterpriseai/definition/natural-language-generation-NLG).
  - [Natural language generation - Wikipedia](https://en.wikipedia.org/wiki/Natural_language_generation).
- **Token**
  - Tokens are the basic units of data processed by LLMs. In the context of text, a token can be a word, part of a word (subword), or even a character — depending on the tokenization process. [The Building Blocks of LLMs: Vectors, Tokens and Embeddings - The New Stack](https://thenewstack.io/the-building-blocks-of-llms-vectors-tokens-and-embeddings/).
- **RL (Reinforcement Learning)**
  - [Reinforcement learning - Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning).
- **BERT (Bidirectional Encoder Representations from Transformers)**
  - [What is BERT (Language Model) and How Does It Work?](https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model)
  - [BERT Explained: State of the art language model for NLP - Towards Data Science](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270).
  - [BERT (language model) - Wikipedia](https://en.wikipedia.org/wiki/BERT_(language_model)).
- **SFT (Supervised Fine-Tuning)**
  - [Supervised Fine-tuning: customizing LLMs - Medium](https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3).
- **RLHF (Reinforcement Learning from Human Feedback)**
  - [ChatGPT: Training - Wikipedia](https://en.wikipedia.org/wiki/ChatGPT#Training).
  - [RLHF + Reward Model + PPO on LLMs - Medium](https://medium.com/@madhur.prashant7/rlhf-reward-model-ppo-on-llms-dfc92ec3885f).
  - **RM (Reward Model)**
    - [RLHF + Reward Model + PPO on LLMs - Medium](https://medium.com/@madhur.prashant7/rlhf-reward-model-ppo-on-llms-dfc92ec3885f).
  - **PPO (Proximal Policy Optimizer)**
    - [RLHF + Reward Model + PPO on LLMs - Medium](https://medium.com/@madhur.prashant7/rlhf-reward-model-ppo-on-llms-dfc92ec3885f).
    - [Proximal Policy Optimization - OpenAI](https://openai.com/research/openai-baselines-ppo).
- **SSF (Scaling & Shifting Your Features)**
  -  [Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning.pdf](https://openreview.net/pdf?id=XtyeppctGgc).
- **AIGC (Artificial Intelligence Generated Content)**
  - [AI-Generated Content and ChatGPT: A Complete Guide](https://www.conductor.com/academy/ai-generated-content/).
  - By comparison to **UGC (User Generated Content)**, **Professionally Generated Content (PGC)**, and **OGC (Occupationally Generated Content)**: [Full article: New perspective on UGC, PGC, and OGC: motivating factors of Chinese co-creators’ engagement with international television series](https://www.tandfonline.com/doi/full/10.1080/17510694.2022.2150816).
- **Hallucination in AI** (or artificial hallucination, confabulation, delusion): In NLP, a hallucination is often defined as "generated content that appears factual but is ungrounded". This term draws a loose analogy with human psychology ([Hallucination - Wikipedia](https://en.wikipedia.org/wiki/Hallucination)), where hallucination typically involves false percepts.
  - [ChatGPT - Wikipedia](https://en.wikipedia.org/wiki/ChatGPT).
  - [Why large language models like ChatGPT are bullshit artists \| by Lak Lakshmanan \| Dec, 2022 \| Becoming Human: Artificial Intelligence Magazine](https://web.archive.org/web/20221217075021/https://becominghuman.ai/why-large-language-models-like-chatgpt-are-bullshit-artists-c4d5bb850852)
  - [Hallucination (artificial intelligence) - Wikipedia](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)).
  - Tonmoy, S. M., et al. "A comprehensive survey of hallucination mitigation techniques in large language models." *arXiv preprint arXiv:2401.01313* (2024), available at: [[2401.01313] A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models](https://arxiv.org/abs/2401.01313).
- **AI copilot**
  - AI copilot is a conversational interface that uses large language models (LLMs) to support users in various tasks and decision-making processes across multiple domains within an enterprise environment. By leveraging LLMs, AI copilots possess the capability to understand, analyze, and process vast amounts of data. [AI Copilots: What are they and how do they work? \| Moveworks](https://www.moveworks.com/us/en/resources/blog/what-is-an-ai-copilot).
  - AI copilots offer a new and different opportunity to reimagine the value that software can deliver to users. Unlike earlier chatbots, AI copilots use powerful LLMs like GPT-4 to answer questions, provide insights, and assist users across the application. They deeply understand how to use an application’s data and APIs and can be integrated across an application as both a conversational copilot and copilot-powered features and services. [What is an AI copilot? - Continual](https://www.continual.ai/blog-post/what-is-an-ai-copilot).
- **AI Alignment**
  - Alignment is the process of encoding human values and goals into large language models to make them as helpful, safe, and reliable as possible. Through alignment, enterprises can tailor AI models to follow their business rules and policies. [What is AI alignment? - IBM Research](https://research.ibm.com/blog/what-is-alignment-ai).
  - Evaluating AI alignment: If AI progress continues, AI systems will eventually possess highly dangerous capabilities. Before training and deploying such systems, we need methods to assess their propensity to use these capabilities. Purely behavioral evaluations may fail for advanced AI systems: Similar to humans, they might behave differently under evaluation, faking alignment. *Managing extreme AI risks amid rapid progress*, Yoshua Bengio, Geoffrey Hinton, and Andrew Yao *et al*. [Managing extreme AI risks amid rapid progress \| Science](https://www.science.org/doi/10.1126/science.adn0117).
  - In the opposite direction, there is ChatGPT DAN (Do Anything Now) mode (that is jailbreak version designed to test the limits of ChatGPT). [How to Use ChatGPT Dan - Detailed Guide](https://whatsthebigdata.com/chatgpt-dan/).
- **LoRA (Low-Rank Adaptation)**
  - Low-Rank Adaptation aka LoRA is a technique used to finetuning LLMs in a parameter efficient way. This doesn't involve finetuning whole of the base model, which can be huge and cost a lot of time and money. LoRA, instead adds a small number of trainable parameters to the model while keeping the original model parameters frozen. [A beginners guide to fine tuning LLM using LoRA](https://zohaib.me/a-beginners-guide-to-fine-tuning-llm-using-lora/).
  - QLoRA: [[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314).
  - DeepSpeed**: [microsoft/DeepSpeed: DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.](https://github.com/microsoft/DeepSpeed).

- **Megatron-LM**
  - Megatron-LM serves as a ressearch-oriented framework leveraging Megatron-Core for large language model (LLM) training. [NVIDIA/Megatron-LM: Ongoing research training transformer models at scale](https://github.com/NVIDIA/Megatron-LM).

- **FlashAttention**
  - Dao, Tri, et al. "Flashattention: Fast and memory-efficient exact attention with io-awareness." *Advances in Neural Information Processing Systems* 35 (2022): 16344-16359, available at: [Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://github.com/Dao-AILab/flash-attention).
  - [Dao-AILab/flash-attention: Fast and memory-efficient exact attention](https://github.com/Dao-AILab/flash-attention).

- **Direct Preference Optimization (DPO)**
  - Direct Preference Optimization (DPO) is a stable, performant, and computationally lightweight, technique for aligning LLM’s with a simple classification loss. DPO eliminates the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. [Aligning LLMs with Direct Preference Optimization (DPO)— background, overview, intuition and paper summary \| by Manish Chablani \| Medium](https://medium.com/@ManishChablani/aligning-llms-with-direct-preference-optimization-dpo-background-overview-intuition-and-paper-0a72b9dc539c).
  - Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." *Advances in Neural Information Processing Systems* 36 (2024), available at: [[2305.18290] Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290).


<br>
